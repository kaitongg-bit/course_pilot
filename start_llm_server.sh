#!/bin/bash

echo "ğŸš€ å¯åŠ¨æœ¬åœ°LLMæœåŠ¡å™¨..."

# Always run relative to this script's directory
cd "$(dirname "$0")" || exit 1

# Ensure venv exists
if [ ! -d "venv" ]; then
    echo "âŒ æœªæ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒ venvï¼Œè¯·å…ˆè¿è¡Œ install_local_llm.sh"
    exit 1
fi

# Activate venv
echo "ğŸ“¦ æ­£åœ¨æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ..."
source venv/bin/activate

# Check llama-cpp-python availability
python3 - << 'EOF'
import sys
try:
    import llama_cpp
except Exception as e:
    print("âŒ é”™è¯¯ï¼šllama-cpp-python æœªå®‰è£…åœ¨ venv ä¸­ï¼Œè¯·è¿è¡Œ install_local_llm.sh")
    sys.exit(1)
EOF

echo "âœ¨ å¯åŠ¨ Python åç«¯..."

# Use the correct backend filename (with hyphen)
python3 backend/llm-proxy.py

